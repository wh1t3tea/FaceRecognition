{"cells":[{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T01:58:13.720356Z","iopub.status.busy":"2024-04-10T01:58:13.719502Z","iopub.status.idle":"2024-04-10T01:58:38.056803Z","shell.execute_reply":"2024-04-10T01:58:38.055653Z","shell.execute_reply.started":"2024-04-10T01:58:13.720323Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pytorch-metric-learning in /opt/conda/lib/python3.10/site-packages (2.5.0)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pytorch-metric-learning) (1.26.4)\n","Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from pytorch-metric-learning) (1.2.2)\n","Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-metric-learning) (2.1.2)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from pytorch-metric-learning) (4.66.1)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->pytorch-metric-learning) (3.13.1)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->pytorch-metric-learning) (4.9.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->pytorch-metric-learning) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->pytorch-metric-learning) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->pytorch-metric-learning) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->pytorch-metric-learning) (2024.2.0)\n","Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pytorch-metric-learning) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pytorch-metric-learning) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pytorch-metric-learning) (3.2.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->pytorch-metric-learning) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->pytorch-metric-learning) (1.3.0)\n","Requirement already satisfied: ellzaf_ml in /opt/conda/lib/python3.10/site-packages (1.4.14)\n","Requirement already satisfied: einops>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from ellzaf_ml) (0.7.0)\n","Requirement already satisfied: torch>=1.10 in /opt/conda/lib/python3.10/site-packages (from ellzaf_ml) (2.1.2)\n","Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from ellzaf_ml) (0.16.2)\n","Requirement already satisfied: mediapipe in /opt/conda/lib/python3.10/site-packages (from ellzaf_ml) (0.10.11)\n","Requirement already satisfied: opencv-python in /opt/conda/lib/python3.10/site-packages (from ellzaf_ml) (4.9.0.80)\n","Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from ellzaf_ml) (1.11.4)\n","Requirement already satisfied: timm>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from ellzaf_ml) (0.9.16)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm>=0.9.0->ellzaf_ml) (6.0.1)\n","Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (from timm>=0.9.0->ellzaf_ml) (0.20.3)\n","Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm>=0.9.0->ellzaf_ml) (0.4.2)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->ellzaf_ml) (3.13.1)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->ellzaf_ml) (4.9.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->ellzaf_ml) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->ellzaf_ml) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->ellzaf_ml) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->ellzaf_ml) (2024.2.0)\n","Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from mediapipe->ellzaf_ml) (1.4.0)\n","Requirement already satisfied: attrs>=19.1.0 in /opt/conda/lib/python3.10/site-packages (from mediapipe->ellzaf_ml) (23.2.0)\n","Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from mediapipe->ellzaf_ml) (23.5.26)\n","Requirement already satisfied: jax in /opt/conda/lib/python3.10/site-packages (from mediapipe->ellzaf_ml) (0.4.23)\n","Requirement already satisfied: jaxlib in /opt/conda/lib/python3.10/site-packages (from mediapipe->ellzaf_ml) (0.4.23.dev20240116)\n","Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from mediapipe->ellzaf_ml) (3.7.5)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from mediapipe->ellzaf_ml) (1.26.4)\n","Requirement already satisfied: opencv-contrib-python in /opt/conda/lib/python3.10/site-packages (from mediapipe->ellzaf_ml) (4.9.0.80)\n","Requirement already satisfied: protobuf<4,>=3.11 in /opt/conda/lib/python3.10/site-packages (from mediapipe->ellzaf_ml) (3.20.3)\n","Requirement already satisfied: sounddevice>=0.4.4 in /opt/conda/lib/python3.10/site-packages (from mediapipe->ellzaf_ml) (0.4.6)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->ellzaf_ml) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->ellzaf_ml) (9.5.0)\n","Requirement already satisfied: CFFI>=1.0 in /opt/conda/lib/python3.10/site-packages (from sounddevice>=0.4.4->mediapipe->ellzaf_ml) (1.16.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm>=0.9.0->ellzaf_ml) (4.66.1)\n","Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm>=0.9.0->ellzaf_ml) (21.3)\n","Requirement already satisfied: ml-dtypes>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from jax->mediapipe->ellzaf_ml) (0.2.0)\n","Requirement already satisfied: opt-einsum in /opt/conda/lib/python3.10/site-packages (from jax->mediapipe->ellzaf_ml) (3.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10->ellzaf_ml) (2.1.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe->ellzaf_ml) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe->ellzaf_ml) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe->ellzaf_ml) (4.47.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe->ellzaf_ml) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe->ellzaf_ml) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe->ellzaf_ml) (2.8.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->ellzaf_ml) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->ellzaf_ml) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->ellzaf_ml) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->ellzaf_ml) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10->ellzaf_ml) (1.3.0)\n","Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe->ellzaf_ml) (2.21)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe->ellzaf_ml) (1.16.0)\n"]}],"source":["!pip install pytorch-metric-learning\n","#!pip install facenet_pytorch\n","!pip install ellzaf_ml\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch import nn\n","from torchvision import transforms as tfs\n","from torch.utils.data import DataLoader, Dataset\n","from PIL import Image\n","import cv2\n","from pytorch_metric_learning.losses import ArcFaceLoss\n","#from pytorch_metric_learning.miners import TripletMarginMiner, BatchHardMiner, MultiSimilarityMiner\n","#from pytorch_metric_learning import regularizers\n","#from pytorch_metric_learning.distances import LpDistance, CosineSimilarity\n","#from facenet_pytorch import fixed_image_standardization\n","import os\n","import matplotlib.pyplot as plt\n","import torch\n","from ellzaf_ml.models import GhostFaceNetsV2\n","import numpy as np\n","np.bool = np.bool_\n","import torch\n","from functools import partial\n","from torch import distributed\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import transforms as tfs\n","from torchvision.datasets import ImageFolder\n","import os\n","from tqdm import tqdm_notebook as tqdm\n","from torch.nn.functional import cosine_similarity\n","import torch.nn.functional as F\n","#from pytorch_metric_learning.samplers import MPerClassSampler\n","from torch.utils.data import WeightedRandomSampler\n","import time\n","from torch.utils.data.dataset import random_split"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T01:59:17.785658Z","iopub.status.busy":"2024-04-10T01:59:17.784727Z","iopub.status.idle":"2024-04-10T01:59:17.795539Z","shell.execute_reply":"2024-04-10T01:59:17.794536Z","shell.execute_reply.started":"2024-04-10T01:59:17.785624Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Random seed set as 42\n"]}],"source":["import random\n","def set_seed(seed: int = 42) -> None:\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    # When running on the CuDNN backend, two further options must be set\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    # Set a fixed value for the hash seed\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    print(f\"Random seed set as {seed}\")\n","    \n","set_seed(42)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class CasiaSampler:\n","    def __init__(self, data):\n","        self.targets = data.targets\n","        self.labels = data.classes\n","        self.weights = self.compute_weights(self.targets)\n","        \n","    def compute_weights(self, targets):\n","        class_sample_count = np.array([len(np.where(targets == t)[0]) for t in np.unique(targets)])\n","        weight = 1. / class_sample_count\n","        samples_weight = np.array([weight[t] for t in targets])\n","        samples_weight = torch.from_numpy(samples_weight)\n","        samples_weight = samples_weight.double()\n","        return samples_weight\n","        \n","    def get_sampler(self):\n","        weights = self.weights\n","        return WeightedRandomSampler(weights, len(weights))\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def compute_weights(targets):\n","        class_sample_count = np.array([len(np.where(targets == t)[0]) for t in np.unique(targets)])\n","        weight = 1. / class_sample_count\n","        samples_weight = np.array([weight[t] for t in targets])\n","        samples_weight = torch.from_numpy(samples_weight)\n","        samples_weight = samples_weight.double()\n","        return samples_weight"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-01T13:02:38.728640Z","iopub.status.busy":"2024-04-01T13:02:38.728348Z","iopub.status.idle":"2024-04-01T13:34:05.217238Z","shell.execute_reply":"2024-04-01T13:34:05.216392Z","shell.execute_reply.started":"2024-04-01T13:02:38.728614Z"},"trusted":true},"outputs":[],"source":["import time\n","\n","train_root = '/kaggle/input/ms1m-retinaface/ms1m-retinaface-t1/imgs'\n","test_root = '/kaggle/input/lfw-aligned/kaggle/working/lfw-aligned'\n","\n","transforms_train = tfs.Compose(\n","            [tfs.ToTensor(),\n","             tfs.RandomHorizontalFlip(),\n","             tfs.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n","             ])\n","\n","transforms_test = tfs.Compose(\n","            [tfs.ToTensor(),\n","             tfs.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n","             ])\n","\n","train_data = ImageFolder(train_root, transform=transforms_train)\n","test_data = ImageFolder(test_root, transform=transforms_test)\n","batch_size = 512\n","\n","trainloader = DataLoader(train_data,\n","                         batch_size=batch_size,\n","                         shuffle=True,\n","                         pin_memory=True,\n","                         num_workers=4,\n","                         drop_last=True)\n","testloader = DataLoader(test_data,\n","                        batch_size=batch_size,\n","                        pin_memory=True,\n","                        num_workers=4,\n","                        drop_last=True)"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2024-04-01T18:31:01.947282Z","iopub.status.busy":"2024-04-01T18:31:01.946918Z","iopub.status.idle":"2024-04-01T18:31:01.952450Z","shell.execute_reply":"2024-04-01T18:31:01.951405Z","shell.execute_reply.started":"2024-04-01T18:31:01.947257Z"},"trusted":true},"outputs":[],"source":["batch_size = 256\n","\n","trainloader = DataLoader(train_data,\n","                         batch_size=batch_size,\n","                         shuffle=True,\n","                         pin_memory=True,\n","                         num_workers=4,\n","                         drop_last=True)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T01:59:22.714941Z","iopub.status.busy":"2024-04-10T01:59:22.714554Z","iopub.status.idle":"2024-04-10T01:59:22.723339Z","shell.execute_reply":"2024-04-10T01:59:22.722426Z","shell.execute_reply.started":"2024-04-10T01:59:22.714912Z"},"trusted":true},"outputs":[],"source":["class BaseDataset(Dataset):\n","    def __init__(self, data_path, pairs_list, root_dir):\n","        super().__init__()\n","        self.data_path = data_path\n","        self.pairs_list = pairs_list\n","        self.transform = tfs.Compose([\n","            tfs.ToTensor(),\n","            tfs.Resize((112, 112)),\n","            tfs.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n","        ])\n","        self.root_dir = root_dir\n","        \n","    def __len__(self):\n","        return len(self.pairs_list)\n","    \n","    def __getitem__(self, idx):\n","        img_name1, img_name2, issame = self.pairs_list.iloc[idx]\n","        img1 = Image.open(os.path.join(self.root_dir, img_name1))\n","        img2 = Image.open(os.path.join(self.root_dir, img_name2))\n","        img1 = self.transform(img1)\n","        img2 = self.transform(img2)\n","        return img1, img2, int(issame)"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T02:24:26.936233Z","iopub.status.busy":"2024-04-10T02:24:26.935762Z","iopub.status.idle":"2024-04-10T02:24:26.963801Z","shell.execute_reply":"2024-04-10T02:24:26.962553Z","shell.execute_reply.started":"2024-04-10T02:24:26.936195Z"},"trusted":true},"outputs":[],"source":["class Evaluate:\n","    def __init__(self, data_path, pairs_path, root_dir, size):\n","        \"\"\"\n","        data_path: {str} path to the image folder.\n","        pairs_path: {str} path to the pairs list.\n","        size: {int} number related to set size:\n","                        {\n","                        0: 25% dataset size\n","                        1: 50% dataset size\n","                        2: 75% dataset size\n","                        3: 100% dataset size\n","                        None: equals to 3 (100% size)\n","                        }\n","        \"\"\"\n","        self.root_dir = r'/kaggle/input/lwf-eval/lfw/lfw'\n","        self.data_path = data_path\n","        self.pairs_path = pairs_path\n","        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","        available_sizes = [None, 0, 1, 2, 3]\n","        if size == None:\n","            size = 3\n","        self.size = size\n","        self.dataset = self.get_dataset(self.data_path,\n","                                        self.pairs_path,\n","                                        self.size)\n","        self.dataloader = self.get_dataloader(self.dataset)\n","        self.verbose = 1\n","        \n","    def get_dataset(self, data_path, pairs_path, size):\n","        pairs = pd.read_csv(pairs_path, sep=' ', names=['First_image', 'Second_image', 'Issame'])\n","        data = BaseDataset(data_path, pairs, self.root_dir)\n","        return data\n","    \n","    def get_dataloader(self, data, batch_size=32):\n","        dataloader = DataLoader(data, batch_size=32, pin_memory=True, num_workers=0, drop_last=True)\n","        return dataloader\n","    \n","    def compute_threshold(self, model, fpr=0.25):\n","        id_rate = IdRate()\n","        metric, threshold = id_rate.id_rate(model, fpr)\n","        print(f'TPR@FPR={fpr}: {metric}')\n","        return threshold\n","    \n","    def accuracy(self, model, size, metrics):\n","        \n","        \"\"\"\n","        threshold: {float} computed with TPR@FPR=0.05 metric.\n","        size: {int} dissipate in quarters.\n","        metric: {str} any of {\"accuracy\", \"f1-score\", \"precision\", \"recall\"}\n","        \"\"\"\n","        threshold = self.compute_threshold(model, 0.02)\n","        batch_res = {'tp': 0,\n","                     'tn': 0,\n","                     'fp': 0,\n","                     'fn': 0}\n","        for img1, img2, issame in self.dataloader:\n","            with torch.inference_mode():\n","                model.eval()\n","                img1, img2 = img1.to(self.device), img2.to(self.device)\n","                embd1 = F.normalize(model(img1))\n","                embd2 = F.normalize(model(img2))\n","                cos_dist = 1 - cosine_similarity(embd1, embd2, dim=1)\n","                labels = torch.tensor([1 if cos_dist[idx] <= threshold else -1 for idx in range(32)])\n","                tp = sum([1 if issame[idx] == labels[idx] == 1 else 0 for idx in range(32)])\n","                tn = sum([1 if issame[idx] == labels[idx] == -1 else 0 for idx in range(32)])\n","                fn = sum([1 if issame[idx] == 1 and labels[idx] == -1 else 0 for idx in range(32)])\n","                fp = sum([1 if issame[idx] == -1 and labels[idx] == 1 else 0 for idx in range(32)])\n","                batch_res['tp'] += tp\n","                batch_res['fp'] += fp\n","                batch_res['tn'] += tn\n","                batch_res['fn'] += fn\n","        results = {}\n","        for metric in metrics:\n","            if metric not in results.keys():\n","                results[metric] = 0\n","            if metric == 'accuracy':\n","                results['accuracy'] = (batch_res['tp'] + batch_res['tn']) / (batch_res['tp'] + batch_res['tn'] + batch_res['fn'] + batch_res['fp'])\n","            if metric == 'precision':\n","                if (batch_res['tp'] + batch_res['fp']) == 0:\n","                    results['precision'] = 0\n","                else:\n","                    results['precision'] = batch_res['tp'] / (batch_res['tp'] + batch_res['fp'])\n","            if metric == 'recall':\n","                if (batch_res['tp'] + batch_res['fn']) == 0:\n","                    results['recall'] = 0\n","                else:\n","                    results['recall'] = batch_res['tp'] / (batch_res['tp'] + batch_res['fn'])\n","            if metric == 'f1_score':\n","                precision = batch_res['tp'] / (batch_res['tp'] + batch_res['fp'])\n","                recall = batch_res['tp'] / (batch_res['tp'] + batch_res['fn'])\n","                if precision + recall == 0:\n","                    results['f1_score'] = 0\n","                else:\n","                    results['f1_score'] = (2 * precision * recall) / (precision + recall)\n","        if self.verbose == 1:\n","            print(f\"Results: {' | '.join(f'{k}: {v}' for k, v in results.items())}\\nTreshold: {np.round(threshold, decimals=5)}\")\n","        return results['accuracy']\n","                        "]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T02:24:28.720375Z","iopub.status.busy":"2024-04-10T02:24:28.719969Z","iopub.status.idle":"2024-04-10T02:24:28.757095Z","shell.execute_reply":"2024-04-10T02:24:28.756114Z","shell.execute_reply.started":"2024-04-10T02:24:28.720345Z"},"trusted":true},"outputs":[],"source":["class IdRate:\n","    def __init__(self, annot_path=None, distractor_path=None):\n","        if annot_path == None:\n","            self.annot_path = '/kaggle/input/celeba-id/kaggle/working/query_anno.txt'\n","        else:\n","            self.annot_path = annot_path\n","        self.distractors_path = r'/kaggle/input/celeba-id/kaggle/working/distractors_' if distractor_path == None else distractor_path\n","        self.distractors_img_names = self.get_distractors_names(self.distractors_path)\n","        self.query_img_names = self.get_query_igm_names(self.annot_path)\n","    \n","    def get_query_igm_names(self, annot_path):\n","        with open(annot_path, 'r') as f:\n","            query_lines = f.readlines()[1:]\n","            query_lines = [x.strip().split(',') for x in query_lines]\n","            query_img_names = [x[0] for x in query_lines]\n","            return query_img_names\n","        \n","    def get_query_dict(self, annot_path):\n","        with open(annot_path, 'r') as f:\n","            query_lines = f.readlines()[1:]\n","            query_lines = [x.strip().split(',') for x in query_lines]\n","            query_img_names = [x[0] for x in query_lines]\n","            query_dict = defaultdict(list)\n","            for img_name, img_class in query_lines:\n","                query_dict[img_class].append(img_name)\n","            return query_dict\n","        \n","    def get_distractors_names(self, path):\n","        return os.listdir(path)\n","    \n","    def preprocess_image(self, img_path):\n","        img = Image.open(img_path)\n","        transform = tfs.Compose([\n","            tfs.ToTensor(),\n","            tfs.Resize((112, 112)),\n","            tfs.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n","        ])\n","        img = transform(img)\n","        return img\n","    \n","    def compute_embeddings(self, model, images_list, root_dir, normal=True):\n","        '''\n","        compute embeddings from the trained model for list of images.\n","        params:\n","          model: trained nn model that takes images and outputs embeddings\n","          images_list: list of images paths to compute embeddings for\n","        output:\n","          list: list of model embeddings. Each embedding corresponds to images\n","                names from images_list\n","        '''\n","        embds = []\n","        with torch.inference_mode():\n","            for img_path in images_list:\n","                model.eval()\n","                img_path_r = os.path.join(root_dir, img_path)\n","                img = self.preprocess_image(img_path_r).to(device)\n","                if normal:\n","                    embd = F.normalize(model(img.unsqueeze(0))).to('cpu')\n","                else:\n","                    embd = model(img.unsqueeze(0)).to('cpu')\n","                embds.append((embd, img_path))\n","        return embds\n","    \n","    def compute_cosine_query_pos(self, query_dict, query_embeddings):\n","        '''\n","        compute cosine similarities between positive pairs from query (stage 1)\n","        params:\n","          query_dict: dict {class: [image_name_1, image_name_2, ...]}. Key: class in\n","                      the dataset. Value: images corresponding to that class\n","          query_img_names: list of images names\n","          query_embeddings: list of embeddings corresponding to query_img_names\n","        output:\n","          list of floats: similarities between embeddings corresponding\n","                          to the same people from query list\n","        '''\n","        grouped_embds = [[0] for x in range(len(query_dict.values()))]\n","        embd_names = [x[1] for x in query_embeddings]\n","        cos_list = []\n","        embds = [x[0] for x in query_embeddings]\n","        for idx, values in enumerate(query_dict.values()):\n","            for value in values:\n","                if value in embd_names:\n","                    if type(grouped_embds[idx][0]) != torch.Tensor:\n","                        grouped_embds[idx] = [query_embeddings[embd_names.index(value)][0]]\n","                    else:\n","                        grouped_embds[idx].append(query_embeddings[embd_names.index(value)][0])\n","        grouped_embds = [x for x in grouped_embds if len(x) > 1]\n","        for group in grouped_embds:\n","            embed = group[0]\n","            for embedding in group[1:]:\n","                cos_list.append(1 - cosine_similarity(embed, embedding))\n","        return cos_list\n","\n","\n","    def compute_cosine_query_neg(self, query_dict, query_embeddings):\n","        '''\n","        compute cosine similarities between negative pairs from query (stage 2)\n","        params:\n","          query_dict: dict {class: [image_name_1, image_name_2, ...]}. Key: class in\n","                      the dataset. Value: images corresponding to that class\n","          query_img_names: list of images names\n","          query_embeddings: list of embeddings corresponding to query_img_names\n","        output:\n","          list of floats: similarities between embeddings corresponding\n","                          to different people from query list\n","        '''\n","        grouped_embds = [[0] for x in range(len(query_dict.values()))]\n","        embd_names = [x[1] for x in query_embeddings]\n","        cos_list = []\n","        embds = [x[0] for x in query_embeddings]\n","        for idx, values in enumerate(query_dict.values()):\n","            for value in values:\n","                if value in embd_names:\n","                    if type(grouped_embds[idx][0]) != torch.Tensor:\n","                        grouped_embds[idx] = [query_embeddings[embd_names.index(value)][0]]\n","                    else:\n","                        grouped_embds[idx].append(query_embeddings[embd_names.index(value)][0])\n","        grouped_embds = [x for x in grouped_embds if len(x) > 1]\n","        tensor_embds = torch.stack(embds)\n","        for group in grouped_embds:\n","            for embedding in group:\n","                mask = torch.any(tensor_embds != embedding, dim=-1)\n","                embds_ = tensor_embds[mask]\n","                cos_list.append(1 - cosine_similarity(embedding.to(device), embds_.to(device)).to('cpu'))\n","        return torch.cat(cos_list)\n","\n","\n","    def compute_cosine_query_distractors(self, query_embeddings, distractors_embeddings):\n","        '''\n","        compute cosine similarities between negative pairs from query and distractors\n","        (stage 3)\n","        params:\n","          query_embeddings: list of embeddings corresponding to query_img_names\n","          distractors_embeddings: list of embeddings corresponding to distractors_img_names\n","        output:\n","          list of floats: similarities between pairs of people (q, d), where q is\n","                          embedding corresponding to photo from query, d â€”\n","                          embedding corresponding to photo from distractors\n","        '''\n","        cos_list = []\n","        distractors_embeddings = torch.stack([x[0] for x in distractors_embeddings])\n","        for q_emb in query_embeddings:\n","            dist_embds_mask = torch.any(distractors_embeddings != q_emb[0], dim=-1)\n","            cos_list.append(\n","                1 - cosine_similarity(q_emb[0].to(device), distractors_embeddings[dist_embds_mask].to(device)).to('cpu'))\n","        return torch.cat(cos_list)\n","    \n","    def compute_ir(self, cosine_query_pos, cosine_query_neg, cosine_query_distractors, fpr=0.1):\n","        '''\n","        compute identification rate using precomputed cosine similarities between pairs\n","        at a given fpr\n","        params:\n","          cosine_query_pos: cosine similarities between positive pairs from query\n","          cosine_query_neg: cosine similarities between negative pairs from query\n","          cosine_query_distractors: cosine similarities between negative pairs\n","                                    from query and distractors\n","          fpr: false positive rate at which to compute TPR\n","        output:\n","          float: threshold for given fpr\n","          float: TPR at given FPR\n","        '''\n","        # Combine negative pairs from query and distractors\n","        all_neg_cos = torch.cat([torch.tensor(cosine_query_distractors), torch.tensor(cosine_query_neg)])\n","\n","        # Find the threshold efficiently without sorting the entire list\n","        if isinstance(fpr, list):\n","            tprs = {}\n","            thresholds = []\n","            for fpr_ in fpr:\n","                threshold_idx = int(fpr_ * len(all_neg_cos))\n","                threshold, _ = torch.kthvalue(all_neg_cos, threshold_idx)\n","\n","                # Convert the threshold to a Python float\n","                threshold = threshold.item()\n","\n","                # Calculate TPR at the given FPR\n","                true_positives = sum(1 for x in cosine_query_pos if x < threshold)\n","                tpr = true_positives / len(cosine_query_pos)\n","                tprs[fpr_] = tpr\n","                thresholds.append(threshold)\n","            return tprs, thresholds\n","\n","        threshold_idx = int(fpr * len(all_neg_cos))\n","        threshold, _ = torch.kthvalue(all_neg_cos, threshold_idx)\n","\n","        # Convert the threshold to a Python float\n","        threshold = threshold.item()\n","\n","        # Calculate TPR at the given FPR\n","        true_positives = sum(1 for x in cosine_query_pos if x < threshold)\n","        tpr = true_positives / len(cosine_query_pos)\n","\n","        return tpr, threshold\n","    \n","    def id_rate(self, model, fpr):\n","        q_embeddings = self.compute_embeddings(model, self.query_img_names,\n","                                          r'/kaggle/input/celeba-id/kaggle/working/celeba_aligned')\n","        d_embeddings = self.compute_embeddings(model, self.distractors_img_names,\n","                                          r'/kaggle/input/celeba-id/kaggle/working/distractors_')\n","        cos_query_pos = self.compute_cosine_query_pos(self.get_query_dict(self.annot_path),\n","                                                 q_embeddings)\n","        cos_query_neg = self.compute_cosine_query_neg(self.get_query_dict(self.annot_path),\n","                                                 q_embeddings)\n","        cos_query_distractors = self.compute_cosine_query_distractors(q_embeddings, d_embeddings)\n","        print(sum(cos_query_pos)/len(cos_query_pos))\n","        print(sum(cos_query_neg)/(len(cos_query_neg) + 1))\n","        print(f\"TPR@FPR=threshold results: {self.compute_ir(cos_query_pos, cos_query_neg, cos_query_distractors, fpr=[0.01, 0.05, 0.1])}\")\n","        return self.compute_ir(cos_query_pos, cos_query_neg, cos_query_distractors, fpr=fpr)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T01:59:24.544159Z","iopub.status.busy":"2024-04-10T01:59:24.543839Z","iopub.status.idle":"2024-04-10T01:59:24.577453Z","shell.execute_reply":"2024-04-10T01:59:24.576332Z","shell.execute_reply.started":"2024-04-10T01:59:24.544135Z"},"trusted":true},"outputs":[],"source":["__all__ = ['iresnet18', 'iresnet34', 'iresnet50', 'iresnet100', 'iresnet200']\n","using_ckpt = False\n","\n","def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n","    \"\"\"3x3 convolution with padding\"\"\"\n","    return nn.Conv2d(in_planes,\n","                     out_planes,\n","                     kernel_size=3,\n","                     stride=stride,\n","                     padding=dilation,\n","                     groups=groups,\n","                     bias=False,\n","                     dilation=dilation)\n","\n","\n","def conv1x1(in_planes, out_planes, stride=1):\n","    \"\"\"1x1 convolution\"\"\"\n","    return nn.Conv2d(in_planes,\n","                     out_planes,\n","                     kernel_size=1,\n","                     stride=stride,\n","                     bias=False)\n","\n","\n","class IBasicBlock(nn.Module):\n","    expansion = 1\n","    def __init__(self, inplanes, planes, stride=1, downsample=None,\n","                 groups=1, base_width=64, dilation=1):\n","        super(IBasicBlock, self).__init__()\n","        if groups != 1 or base_width != 64:\n","            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n","        if dilation > 1:\n","            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n","        self.bn1 = nn.BatchNorm2d(inplanes, eps=1e-05,)\n","        self.conv1 = conv3x3(inplanes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes, eps=1e-05,)\n","        self.prelu = nn.PReLU(planes)\n","        self.conv2 = conv3x3(planes, planes, stride)\n","        self.bn3 = nn.BatchNorm2d(planes, eps=1e-05,)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward_impl(self, x):\n","        identity = x\n","        out = self.bn1(x)\n","        out = self.conv1(out)\n","        out = self.bn2(out)\n","        out = self.prelu(out)\n","        out = self.conv2(out)\n","        out = self.bn3(out)\n","        if self.downsample is not None:\n","            identity = self.downsample(x)\n","        out += identity\n","        return out        \n","\n","    def forward(self, x):\n","        if self.training and using_ckpt:\n","            return checkpoint(self.forward_impl, x)\n","        else:\n","            return self.forward_impl(x)\n","\n","\n","class IResNet(nn.Module):\n","    fc_scale = 7 * 7\n","    def __init__(self,\n","                 block, layers, dropout=0, num_features=512, zero_init_residual=False,\n","                 groups=1, width_per_group=64, replace_stride_with_dilation=None, fp16=False):\n","        super(IResNet, self).__init__()\n","        self.extra_gflops = 0.0\n","        self.fp16 = fp16\n","        self.inplanes = 64\n","        self.dilation = 1\n","        if replace_stride_with_dilation is None:\n","            replace_stride_with_dilation = [False, False, False]\n","        if len(replace_stride_with_dilation) != 3:\n","            raise ValueError(\"replace_stride_with_dilation should be None \"\n","                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n","        self.groups = groups\n","        self.base_width = width_per_group\n","        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(self.inplanes, eps=1e-05)\n","        self.prelu = nn.PReLU(self.inplanes)\n","        self.layer1 = self._make_layer(block, 64, layers[0], stride=2)\n","        self.layer2 = self._make_layer(block,\n","                                       128,\n","                                       layers[1],\n","                                       stride=2,\n","                                       dilate=replace_stride_with_dilation[0])\n","        self.layer3 = self._make_layer(block,\n","                                       256,\n","                                       layers[2],\n","                                       stride=2,\n","                                       dilate=replace_stride_with_dilation[1])\n","        self.layer4 = self._make_layer(block,\n","                                       512,\n","                                       layers[3],\n","                                       stride=2,\n","                                       dilate=replace_stride_with_dilation[2])\n","        self.bn2 = nn.BatchNorm2d(512 * block.expansion, eps=1e-05,)\n","        self.dropout = nn.Dropout(p=dropout, inplace=True)\n","        self.fc = nn.Linear(512 * block.expansion * self.fc_scale, num_features)\n","        self.features = nn.BatchNorm1d(num_features, eps=1e-05)\n","        nn.init.constant_(self.features.weight, 1.0)\n","        self.features.weight.requires_grad = False\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.normal_(m.weight, 0, 0.1)\n","            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","\n","        if zero_init_residual:\n","            for m in self.modules():\n","                if isinstance(m, IBasicBlock):\n","                    nn.init.constant_(m.bn2.weight, 0)\n","\n","    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n","        downsample = None\n","        previous_dilation = self.dilation\n","        if dilate:\n","            self.dilation *= stride\n","            stride = 1\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                conv1x1(self.inplanes, planes * block.expansion, stride),\n","                nn.BatchNorm2d(planes * block.expansion, eps=1e-05, ),\n","            )\n","        layers = []\n","        layers.append(\n","            block(self.inplanes, planes, stride, downsample, self.groups,\n","                  self.base_width, previous_dilation))\n","        self.inplanes = planes * block.expansion\n","        for _ in range(1, blocks):\n","            layers.append(\n","                block(self.inplanes,\n","                      planes,\n","                      groups=self.groups,\n","                      base_width=self.base_width,\n","                      dilation=self.dilation))\n","\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        with torch.cuda.amp.autocast(self.fp16):\n","            x = self.conv1(x)\n","            x = self.bn1(x)\n","            x = self.prelu(x)\n","            x = self.layer1(x)\n","            x = self.layer2(x)\n","            x = self.layer3(x)\n","            x = self.layer4(x)\n","            x = self.bn2(x)\n","            x = torch.flatten(x, 1)\n","            x = self.dropout(x)\n","        x = self.fc(x.float() if self.fp16 else x)\n","        x = self.features(x)\n","        return x\n","\n","\n","def _iresnet(arch, block, layers, pretrained, progress, **kwargs):\n","    model = IResNet(block, layers, **kwargs)\n","    if pretrained:\n","        raise ValueError()\n","    return model\n","\n","\n","def iresnet18(pretrained=False, progress=True, **kwargs):\n","    return _iresnet('iresnet18', IBasicBlock, [2, 2, 2, 2], pretrained,\n","                    progress, **kwargs)\n","\n","\n","def iresnet34(pretrained=False, progress=True, **kwargs):\n","    return _iresnet('iresnet34', IBasicBlock, [3, 4, 6, 3], pretrained,\n","                    progress, **kwargs)\n","\n","\n","def iresnet50(pretrained=False, progress=True, **kwargs):\n","    return _iresnet('iresnet50', IBasicBlock, [3, 4, 14, 3], pretrained,\n","                    progress, **kwargs)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T01:59:25.877508Z","iopub.status.busy":"2024-04-10T01:59:25.877117Z","iopub.status.idle":"2024-04-10T01:59:25.930917Z","shell.execute_reply":"2024-04-10T01:59:25.930008Z","shell.execute_reply.started":"2024-04-10T01:59:25.877473Z"},"trusted":true},"outputs":[],"source":["img_path = '/kaggle/input/lwf-eval/lfw/lfw'\n","pairs_path = '/kaggle/input/pairs-list/lfw_pair.txt'\n","root_dir = '/kaggle/input/lwf-eval/lfw/lfw'\n","metric = Evaluate(img_path, pairs_path, root_dir, 3)"]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2024-04-01T22:29:24.158797Z","iopub.status.busy":"2024-04-01T22:29:24.158141Z","iopub.status.idle":"2024-04-01T22:29:24.731551Z","shell.execute_reply":"2024-04-01T22:29:24.730775Z","shell.execute_reply.started":"2024-04-01T22:29:24.158764Z"},"trusted":true},"outputs":[],"source":["from collections import defaultdict\n","import os\n"," \n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","model = GhostFaceNetsV2(image_size=112, num_classes=None, dropout=0., width=1.3).to(device)\n","loss_f = ArcFaceLoss(num_classes=num_classes,\n","                              embedding_size=512,\n","                              scale=32).to(device)\n","optimizer = torch.optim.SGD([\n","                {'params': model.parameters()},\n","                {'params': loss_f.parameters()}], lr=0.1, weight_decay=5e-04)"]},{"cell_type":"code","execution_count":62,"metadata":{"execution":{"iopub.execute_input":"2024-04-01T22:29:44.661931Z","iopub.status.busy":"2024-04-01T22:29:44.661510Z","iopub.status.idle":"2024-04-01T22:29:44.673588Z","shell.execute_reply":"2024-04-01T22:29:44.672779Z","shell.execute_reply.started":"2024-04-01T22:29:44.661898Z"},"trusted":true},"outputs":[],"source":["def train_loop(epochs, model, loss, optimizer, trainloader):\n","    \n","    \n","    train_results = {'train_loss': [],\n","                     'validation_loss': [],\n","                     'accuracy': []}\n","    for epoch in range(epochs):\n","        model.train()\n","        ep_loss = 0\n","        iters = 0\n","        for img_batch, label_batch in trainloader:\n","            iters += 1\n","            img_batch, label_batch = img_batch.to(device), label_batch.to(device)\n","            embeddings = model(img_batch)\n","            train_loss = loss(embeddings, label_batch)\n","            train_loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n","            optimizer.step()\n","            ep_loss += train_loss.item()\n","            optimizer.zero_grad()\n","            if iters % 20 == 0:\n","                for g in optimizer.param_groups:\n","                    if g['lr'] > 1e-5:\n","                        g['lr'] = g['lr'] * 0.99\n","                        print(train_loss)\n","            if iters % 1000 == 0:\n","                print(f\"iter: {iters} | loss: {train_loss}\")\n","        #if epoch in [3, 5, 7, 16, 20, 24]:\n","        #    print(f\"Epoch: {epoch + 1}, lr was reduced\")\n","        #    for g in optimizer.param_groups:\n","        #        if g['lr'] > 1e-5:\n","        #            g['lr'] = g['lr'] * 0.1\n","        \n","        train_results['train_loss'].append(ep_loss / len(trainloader))\n","        \n","        ep_loss = 0\n","        with torch.inference_mode():\n","            model.eval()\n","            accuracy = metric.accuracy(model, 3, ['accuracy', 'recall', 'precision', 'f1_score'])\n","            train_results['accuracy'].append(accuracy)\n","        print(f\"Epoch: {epoch + 1} | train_loss: {train_results['train_loss'][-1]} | accuracy: {train_results['accuracy'][-1]}\")\n","        \n","    return train_results"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["res = train_loop(35, model, loss_f, optimizer, trainloader)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-01T08:41:54.852587Z","iopub.status.busy":"2024-04-01T08:41:54.851688Z","iopub.status.idle":"2024-04-01T08:42:19.596102Z","shell.execute_reply":"2024-04-01T08:42:19.595136Z","shell.execute_reply.started":"2024-04-01T08:41:54.852553Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Results: accuracy: 0.9821189839572193 | recall: 0.9746666666666667 | precision: 0.9895093062605753 | f1_score: 0.9820319059613771\n","Treshold: 0.67582\n"]},{"data":{"text/plain":["0.9821189839572193"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["img_path = '/kaggle/input/lwf-eval/lfw/lfw'\n","pairs_path = '/kaggle/input/pairs-list/lfw_pair.txt'\n","root_dir = '/kaggle/input/lwf-eval/lfw/lfw'\n","metric = Evaluate(img_path, pairs_path, root_dir, 3)\n","metric.accuracy(model, 3, ['accuracy', 'recall', 'precision', 'f1_score'])"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T01:59:42.086050Z","iopub.status.busy":"2024-04-10T01:59:42.085408Z","iopub.status.idle":"2024-04-10T01:59:42.092431Z","shell.execute_reply":"2024-04-10T01:59:42.091420Z","shell.execute_reply.started":"2024-04-10T01:59:42.086018Z"},"trusted":true},"outputs":[],"source":["def issame(img1, img2):\n","    transform = tfs.Compose([\n","        tfs.ToTensor(),\n","        tfs.Resize((112, 112)),\n","        tfs.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n","    ])\n","    with torch.inference_mode():\n","        model.eval()\n","        print(transform(img1).unsqueeze(0).shape)\n","        embd1 = model(transform(img1).unsqueeze(0).to(device))\n","        embd2 = model(transform(img2).unsqueeze(0).to(device))\n","    print(F.cosine_similarity(embd1, embd2))"]},{"cell_type":"code","execution_count":71,"metadata":{"execution":{"iopub.execute_input":"2024-03-27T20:28:41.802822Z","iopub.status.busy":"2024-03-27T20:28:41.802425Z","iopub.status.idle":"2024-03-27T20:28:41.906993Z","shell.execute_reply":"2024-03-27T20:28:41.906245Z","shell.execute_reply.started":"2024-03-27T20:28:41.802792Z"},"trusted":true},"outputs":[],"source":["torch.save(model.state_dict(), 'ghostfacev2_0_5.pth')"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":26922,"sourceId":34595,"sourceType":"datasetVersion"},{"datasetId":1341587,"sourceId":2233088,"sourceType":"datasetVersion"},{"datasetId":4505158,"sourceId":7714351,"sourceType":"datasetVersion"},{"datasetId":4530355,"sourceId":7749226,"sourceType":"datasetVersion"},{"datasetId":4532354,"sourceId":7751815,"sourceType":"datasetVersion"},{"datasetId":4532369,"sourceId":7751836,"sourceType":"datasetVersion"},{"datasetId":4533816,"sourceId":7753883,"sourceType":"datasetVersion"},{"datasetId":4544901,"sourceId":7769184,"sourceType":"datasetVersion"},{"datasetId":4568337,"sourceId":7801711,"sourceType":"datasetVersion"},{"datasetId":4572618,"sourceId":7807742,"sourceType":"datasetVersion"},{"datasetId":4580411,"sourceId":7818131,"sourceType":"datasetVersion"},{"datasetId":4600241,"sourceId":7845701,"sourceType":"datasetVersion"},{"datasetId":4600587,"sourceId":7846166,"sourceType":"datasetVersion"},{"datasetId":4600852,"sourceId":7846537,"sourceType":"datasetVersion"},{"datasetId":4600860,"sourceId":7846549,"sourceType":"datasetVersion"},{"datasetId":4622712,"sourceId":7876954,"sourceType":"datasetVersion"},{"datasetId":4682752,"sourceId":7960454,"sourceType":"datasetVersion"},{"datasetId":4682769,"sourceId":7960474,"sourceType":"datasetVersion"},{"datasetId":4689567,"sourceId":7970080,"sourceType":"datasetVersion"},{"datasetId":4705824,"sourceId":7993188,"sourceType":"datasetVersion"},{"datasetId":4708074,"sourceId":7996288,"sourceType":"datasetVersion"},{"datasetId":4764829,"sourceId":8074362,"sourceType":"datasetVersion"},{"datasetId":4767399,"sourceId":8077910,"sourceType":"datasetVersion"},{"datasetId":4767554,"sourceId":8078125,"sourceType":"datasetVersion"},{"datasetId":4767570,"sourceId":8078148,"sourceType":"datasetVersion"}],"dockerImageVersionId":30664,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
